# ðŸ¤– Proyecto de PredicciÃ³n de Demanda con Machine Learning
## Semana 3 - AnÃ¡lisis de Clustering y ClasificaciÃ³n

---

## ðŸ“‘ Tabla de Contenidos
1. [Integrantes](#-integrantes)
2. [DescripciÃ³n General](#-descripciÃ³n-general)
3. [Objetivos del Proyecto](#-objetivos-del-proyecto)
4. [Estructura del Proyecto](#ï¸-estructura-del-proyecto)
5. [Modelos Implementados](#-modelos-implementados)
6. [AnÃ¡lisis de Clustering](#-anÃ¡lisis-de-clustering-detallado)
7. [AnÃ¡lisis de ClasificaciÃ³n](#-anÃ¡lisis-de-clasificaciÃ³n-supervisada)
8. [JustificaciÃ³n MetodolÃ³gica](#-justificaciÃ³n-del-anÃ¡lisis-de-clustering)
9. [CÃ³mo Usar el Proyecto](#-cÃ³mo-usar-el-proyecto)
10. [Conclusiones e Impacto](#-conclusiones-del-anÃ¡lisis-de-clustering)
11. [Contribuciones del Equipo](#-contribuciÃ³n-del-equipo)

---

## ðŸ‘¥ Integrantes
- Joel Cabrera (CoordinaciÃ³n y anÃ¡lisis general)
- Carlos Moyaa (Desarrollo de modelos)
- Andres Sanchez (AnÃ¡lisis y visualizaciÃ³n)
- Maria Maldonado (DocumentaciÃ³n)

## ðŸ“‹ DescripciÃ³n General
Este proyecto implementa un sistema integral de anÃ¡lisis de demanda combinando tÃ©cnicas de **aprendizaje supervisado** (clasificaciÃ³n) y **aprendizaje no supervisado** (clustering). Se analizan diferentes factores que influyen en la demanda como precios, promociones, factores estacionales y segmentos de clientes para:
1. **Predecir** si la demanda serÃ¡ estable, creciente o decreciente
2. **Identificar** grupos homogÃ©neos de productos/tiendas mediante clustering
3. **Segmentar** patrones de demanda para estrategias de negocio diferenciadas

## ðŸŽ¯ Objetivos del Proyecto
* ðŸ“Š Analizar patrones en datos histÃ³ricos de 10,000 registros de ventas
* ðŸ¤– Implementar modelos de Machine Learning supervisados y no supervisados
* ðŸ”„ Comparar el rendimiento de diferentes algoritmos (Ãrboles, SVM, Random Forest, K-Means, DBSCAN)
* ðŸ’¡ Proporcionar insights accionables sobre factores que influyen en la demanda
* ðŸŽ¯ Segmentar clientes/productos para estrategias personalizadas
* ðŸ“ˆ Generar visualizaciones efectivas de patrones complejos

## ðŸ—‚ï¸ Estructura del Proyecto
```
aprendizaje-automatico-semana-3/
â”œâ”€â”€ .git/                          # Control de versiones
â”œâ”€â”€ .vscode/                       # ConfiguraciÃ³n VS Code
â”œâ”€â”€ LICENSE                        # Licencia MIT
â”œâ”€â”€ README.md                      # DocumentaciÃ³n principal
â”‚
â”œâ”€â”€ public/                        # Carpeta de recursos pÃºblicos
â”‚   â””â”€â”€ img/                       # ImÃ¡genes y grÃ¡ficos generados
â”‚
â””â”€â”€ src/                           # ðŸ“‚ CÃ³digo fuente principal
    â”œâ”€â”€ data/
    â”‚   â””â”€â”€ demand_forecasting.csv        # ðŸ“Š Dataset (10,000 registros)
    â”‚
    â”œâ”€â”€ docs/                            # ðŸ“š DocumentaciÃ³n tÃ©cnica
    â”‚   â”œâ”€â”€ Grupo 1 - Taller Colaborativo...pdf
    â”‚   â””â”€â”€ (otros documentos)
    â”‚
    â””â”€â”€ notebooks/                       # ðŸ““ Jupyter Notebooks
        â”œâ”€â”€ EDA.ipynb                    # AnÃ¡lisis Exploratorio
        â”œâ”€â”€ Algoritmos-ML.ipynb          # Modelos de ClasificaciÃ³n
        â”œâ”€â”€ Analisis-No-Supervisado.ipynb  # Clustering
        â””â”€â”€ TallerColaborativo_S3_Grupo1.ipynb  # Trabajo Integrado
```

### ðŸ“Š Archivos Importantes

| Archivo | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `demand_forecasting.csv` | Datos | 10,000 registros de demanda de ventas |
| `EDA.ipynb` | Notebook | AnÃ¡lisis exploratorio de datos |
| `Algoritmos-ML.ipynb` | Notebook | Modelos supervisados (Ãrbol, SVM, Random Forest) |
| `Analisis-No-Supervisado.ipynb` | Notebook | Clustering (K-Means, DBSCAN, PCA, t-SNE) |
| `TallerColaborativo_S3_Grupo1.ipynb` | Notebook | ConsolidaciÃ³n de resultados |

---

## ðŸ“Š Modelos Implementados

### ðŸŽ¯ Modelos de ClasificaciÃ³n (Supervisados)

#### **1. Ãrbol de DecisiÃ³n (Decision Tree Classifier)**
```
ParÃ¡metros:
  - max_depth: Controla profundidad del Ã¡rbol
  - min_samples_split: Muestras mÃ­nimas para dividir nodo
  - criterion: 'gini' o 'entropy'

Complejidad:
  - Entrenamiento: O(n log n)
  - PredicciÃ³n: O(log n)

Cuando usar:
  âœ… Datos pequeÃ±os/medianos
  âœ… Necesitas interpretabilidad
  âœ… Features categÃ³ricas
```

#### **2. Support Vector Machine (SVM)**
```
Variantes:
  - SVC (clasificaciÃ³n)
  - Kernels: linear, rbf (Radial Basis Function), poly

ParÃ¡metros crÃ­ticos:
  - C: RegularizaciÃ³n (menor = mÃ¡s restricciÃ³n)
  - kernel: 'rbf' recomendado para mayorÃ­a de casos
  - gamma: Influencia de cada punto de entrenamiento

Complejidad:
  - Entrenamiento: O(nÂ²) a O(nÂ³) dependiendo kernel
  - PredicciÃ³n: O(n_support)

Cuando usar:
  âœ… Espacios de alta dimensiÃ³n
  âœ… SeparaciÃ³n no lineal
  âœ… Datos mÃ©dicos/crÃ­ticos
```

#### **3. Random Forest**
```
Estructura:
  - n_estimators: NÃºmero de Ã¡rboles (por defecto 100)
  - max_depth: Profundidad mÃ¡xima de cada Ã¡rbol
  - min_samples_split: Muestras para dividir nodo
  - bootstrap: Muestreo con reemplazo

Complejidad:
  - Entrenamiento: O(T Ã— n log n) donde T = n_estimators
  - PredicciÃ³n: O(T Ã— log n)

Cuando usar:
  âœ… Datos medianos/grandes
  âœ… Reduces overfitting automÃ¡ticamente
  âœ… Obtener importancia de features
  âœ… Balance entre precisiÃ³n e interpretabilidad
```

### ðŸ” Modelos de Clustering (No Supervisados)

#### **4. K-Means Clustering**
```
Algoritmo:
  1. Inicializar k centroides aleatoriamente
  2. Asignar puntos al centroide mÃ¡s cercano
  3. Recalcular centroides como promedio de cluster
  4. Repetir hasta convergencia

ParÃ¡metros:
  - n_clusters: NÃºmero de clusters (crÃ­tico)
  - init: 'k-means++' recomendado
  - n_init: NÃºmero de inicializaciones (10 por defecto)
  - max_iter: Iteraciones mÃ¡ximas

Complejidad:
  - Entrenamiento: O(n Ã— k Ã— i Ã— d) donde i = iteraciones, d = dimensiones
  - PredicciÃ³n: O(k Ã— d)

Ventajas:
  âœ… Muy eficiente incluso con datos grandes
  âœ… FÃ¡cil de implementar y entender
  âœ… Escalable a muchas caracterÃ­sticas

Limitaciones:
  âš ï¸ Debe especificar k de antemano
  âš ï¸ Sensible a inicializaciÃ³n (soluciÃ³n: k-means++)
  âš ï¸ Assume clusters esfÃ©ricos
  âš ï¸ No maneja bien outliers
```

#### **5. DBSCAN (Density-Based Spatial Clustering)**
```
Algoritmo:
  1. Para cada punto no visitado:
  2. Si tiene â‰¥ min_samples dentro eps:
  3. Marca como core point, inicia nuevo cluster
  4. Expande cluster a todos densidad-accesibles
  5. Puntos no alcanzables = ruido/outliers

ParÃ¡metros:
  - eps: Radio de vecindad (crÃ­tico, difÃ­cil de elegir)
  - min_samples: Puntos mÃ­nimos en eps para ser core

Complejidad:
  - Con Ã­ndice espacial: O(n log n) a O(nÂ²)

Ventajas:
  âœ… No requiere especificar k
  âœ… Detecta outliers automÃ¡ticamente
  âœ… Clusters de forma arbitraria
  âœ… TeÃ³ricamente bien fundamentado

Limitaciones:
  âš ï¸ ParÃ¡metros eps, min_samples difÃ­ciles de elegir
  âš ï¸ Problemas con varianza de densidad
  âš ï¸ MÃ¡s lento que K-Means
```

#### **6. PCA (Principal Component Analysis)**
```
PropÃ³sito: ReducciÃ³n lineal de dimensionalidad
Proceso:
  1. Centrar datos (media = 0)
  2. Calcular matriz de covarianza
  3. Obtener eigenvectores (direcciones)
  4. Proyectar datos sobre primeros k eigenvectores

Componentes:
  - PC1: Captura mÃ¡xima varianza
  - PC2: Captura segunda mÃ¡xima varianza (ortogonal)
  - ...
  - PCn: Ordenadas por varianza decreciente

InterpretaciÃ³n:
  - Varianza explicada: (Î»i / Î£Î») Ã— 100%
  - Loadings: ContribuciÃ³n de variables originales

Cuando usar:
  âœ… VisualizaciÃ³n en 2D/3D
  âœ… Reducir ruido
  âœ… Acelerar modelos posteriores
  âš ï¸ LimitaciÃ³n: Asume relaciones lineales
```

#### **7. t-SNE (t-Distributed Stochastic Neighbor Embedding)**
```
PropÃ³sito: VisualizaciÃ³n no lineal (superior a PCA)
Algoritmo:
  1. Calcula similaridades por proximidad local
  2. Mapea a espacio 2D/3D preservando estructura local
  3. Usa distribuciÃ³n t de Student para expansiÃ³n

Ventajas sobre PCA:
  âœ… Preserva estructura local (clusters visibles)
  âœ… Separa bien clusters en visualizaciÃ³n
  âœ… Maneja relaciones no lineales

Desventajas:
  âš ï¸ No determinÃ­stico (varÃ­a entre ejecuciones)
  âš ï¸ No es transformaciÃ³n invertible
  âš ï¸ Distancias globales no confiables
  âš ï¸ Lento con datos grandes (n > 50k)
  âš ï¸ ParÃ¡metro perplexity difÃ­cil de elegir

ParÃ¡metros:
  - n_components: 2 o 3 (default 2)
  - perplexity: 5-50 tÃ­picamente (default 30)
  - learning_rate: 10-1000 (default 200)
  - n_iter: Iteraciones (default 1000)
```

---

## ðŸ“ˆ CaracterÃ­sticas Analizadas

### Variables NumÃ©ricas
- **Sales Quantity**: Cantidad de unidades vendidas
- **Price**: Precio unitario del producto

### Variables CategÃ³ricas
- **Product ID**: Identificador Ãºnico del producto
- **Store ID**: Identificador de la tienda
- **Promotions**: Tipo/nivel de promociÃ³n activa
- **Seasonality Factors**: Factor de estacionalidad
- **External Factors**: Factores econÃ³micos/externos
- **Customer Segments**: Segmento de cliente (B2B, Retail, etc.)

### Variable Temporal
- **Date**: Fecha de la transacciÃ³n (extraer: mes, trimestre, dÃ­a semana)

### Variable Objetivo (Target)
- **Demand Trend**: 
  - `Stable`: Demanda sin cambios significativos (~35%)
  - `Increasing`: Demanda en crecimiento (~45%)
  - `Decreasing`: Demanda en declive (~20%)

---

## ðŸ“‹ ComparaciÃ³n de Modelos

### Matriz Comparativa Supervisados

```
Aspecto              Ãrbol DecisiÃ³n   SVM              Random Forest
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PrecisiÃ³n            â­â­â­          â­â­â­â­        â­â­â­â­â­
Interpretabilidad    â­â­â­â­â­      â­â­             â­â­â­
Velocidad            â­â­â­â­â­      â­â­             â­â­â­
Robustez             â­â­â­          â­â­â­â­â­       â­â­â­â­â­
Escalabilidad        â­â­â­â­        â­â­â­           â­â­â­â­
GeneralizaciÃ³n       â­â­            â­â­â­â­â­       â­â­â­â­â­
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Mejor para:
  - RÃ¡pido, interpretable    MÃ¡xima precisiÃ³n     Balance Ã³ptimo
  - Datos pequeÃ±os           Datos medianos       Datos medianos
  - Prototipado rÃ¡pido       Problemas complejos  ProducciÃ³n
```

### Matriz Comparativa No Supervisados

```
Aspecto              K-Means         DBSCAN           PCA     t-SNE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Velocidad            â­â­â­â­â­      â­â­â­           â­â­â­â­  â­
Escalabilidad        â­â­â­â­â­      â­â­â­           â­â­â­â­  â­â­
Calidad Visual       â­â­â­          â­â­â­           â­â­    â­â­â­â­â­
Manejo Outliers      â­              â­â­â­â­â­       N/A     N/A
IntuiciÃ³n            â­â­â­â­â­      â­â­â­           â­â­â­   â­â­
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Mejor para:
  - ProducciÃ³n        Outliers         Linealidad    ExploraciÃ³n
  - Velocidad         Clusters        ReducciÃ³n     Visual
  - Millones datos    irregulares      automÃ¡tica    interpretable
```

---

## ðŸ““ Notebooks del Proyecto

### 1. **EDA.ipynb** ðŸ“Š
AnÃ¡lisis Exploratorio de Datos completo:
* Carga e inspecciÃ³n del dataset
* Limpieza de datos y transformaciones
* EstadÃ­sticas descriptivas
* VisualizaciÃ³n de distribuciones
* Matriz de correlaciÃ³n
* IdentificaciÃ³n de patrones

### 2. **Algoritmos-ML.ipynb** ðŸ¤–
ImplementaciÃ³n de modelos supervisados:
* Pipeline de preprocesamiento
* DivisiÃ³n train/test (80/20)
* **Modelo 1: Ãrbol de DecisiÃ³n** - RÃ¡pido e interpretable
* **Modelo 2: SVM** - Con GridSearchCV para optimizaciÃ³n
* **Modelo 3: Random Forest** - Ensemble robusto
* EvaluaciÃ³n con mÃºltiples mÃ©tricas (Precision, Recall, F1-Score)
* Matrices de confusiÃ³n comparativas

### 3. **Analisis-No-Supervisado.ipynb** ðŸ”
AnÃ¡lisis de clustering y patrones:
* **K-Means**: Con mÃ©todo del codo y silueta
* **DBSCAN**: Con anÃ¡lisis de eps y min_samples
* **PCA**: ReducciÃ³n a 2 dimensiones
* **t-SNE**: VisualizaciÃ³n no lineal
* AnÃ¡lisis de perfiles por cluster
* RelaciÃ³n entre clusters y tendencia de demanda

### 4. **TallerColaborativo_S3_Grupo1.ipynb** ðŸ‘¥
Trabajo colaborativo del grupo:
* AnÃ¡lisis integrado supervisado + no supervisado
* Resultados consolidados del equipo

---

## ðŸ”¬ JustificaciÃ³n del AnÃ¡lisis de Clustering

### Â¿Por quÃ© Clustering?
El anÃ¡lisis de clustering en este proyecto es esencial por las siguientes razones:

#### 1. **Descubrimiento de Patrones Ocultos**
- Los datos de demanda contienen **grupos naturales** de productos/tiendas que no son evidentes
- El clustering permite identificar **segmentos de comportamiento similares** sin etiquetaciÃ³n previa
- Facilita la comprensiÃ³n de la **heterogeneidad en la demanda**

#### 2. **SegmentaciÃ³n para Estrategia Comercial**
- Permite diseÃ±ar **estrategias diferenciadas** por segmento de demanda
- Cada cluster puede requerir **polÃ­ticas de precio, inventario y promociÃ³n distintas**
- OptimizaciÃ³n de recursos asignando esfuerzos a segmentos de mayor impacto

#### 3. **PreparaciÃ³n para ClasificaciÃ³n Mejorada**
- Los clusters identifican **subpoblaciones** dentro de los datos
- Modelar **clasificadores especÃ­ficos por cluster** puede mejorar la precisiÃ³n general
- Reduce la heterogeneidad dentro de conjuntos de entrenamiento

#### 4. **ValidaciÃ³n de HipÃ³tesis**
- Verificar si las tendencias de demanda se alinean con **agrupaciones esperadas**
- Identificar **tendencias anÃ³malas** o grupos inesperados
- Validar supuestos sobre factores que influyen en demanda

### MÃ©todos Seleccionados

| Algoritmo | RazÃ³n | Ventajas | Desventajas |
|-----------|-------|----------|-----------|
| **K-Means** | EstÃ¡ndar y eficiente | RÃ¡pido, interpretable, escala bien | Requiere especificar k, sensible a inicializaciÃ³n |
| **DBSCAN** | Detectar outliers | Agrupa por densidad, sin k predefinido | ParÃ¡metros eps sensibles, varianza de tamaÃ±o |
| **PCA** | ReducciÃ³n visual | 2D/3D para visualizaciÃ³n, reduce ruido | Pierde interpretabilidad, asume linealidad |
| **t-SNE** | Estructura compleja | Preserva estructura local, visualizaciÃ³n, no lineal | Computacionalmente intensivo |

---

## ðŸ“Š AnÃ¡lisis de Clustering Detallado

### Fase 1: SelecciÃ³n del NÃºmero Ã“ptimo de Clusters

#### **MÃ©todo del Codo (Elbow Method)**
```
PropÃ³sito: Encontrar el "codo" en la grÃ¡fica de inercia
CÃ³mo funciona:
  - Se entrenan K-Means con k=1,2,3,...,n
  - Se grafica k vs inercia (suma de distancias intra-cluster)
  - El "codo" (cambio mÃ¡s abrupto) indica k Ã³ptimo
Resultado esperado: Tipicamente 3-5 clusters para datos de demanda
```

#### **AnÃ¡lisis de Silueta (Silhouette Score)**
```
PropÃ³sito: Medir quÃ© tan bien separados estÃ¡n los clusters
Rango: [-1, 1]
  - 1:  Clusters bien definidos
  - 0:  SuperposiciÃ³n entre clusters
  - -1: Puntos mal clasificados
DecisiÃ³n: Elegir k que maximice el score medio de silueta
```

### Fase 2: SegmentaciÃ³n con K-Means

#### **Proceso:**
1. **NormalizaciÃ³n de datos**: StandardScaler para evitar sesgos por escala
2. **InicializaciÃ³n**: k-means++ para evitar Ã³ptimos locales
3. **Entrenamiento**: MÃºltiples corridas para convergencia
4. **AsignaciÃ³n**: Cada registro asignado al centroide mÃ¡s cercano

#### **Esperado:**
- Clusters homogÃ©neos internamente (baja varianza intra-cluster)
- Clusters separados entre sÃ­ (alta varianza inter-cluster)
- Interpretabilidad: Cada cluster representa un **perfil de demanda distinto**

### Fase 3: SegmentaciÃ³n con DBSCAN

#### **Diferencias clave con K-Means:**
```
K-Means:              DBSCAN:
- Particiona forzado  - Basado en densidad
- Todos los puntos    - Identifica outliers
- k predefinido       - ParÃ¡metros eps, min_samples
- Clusters esfÃ©ricos  - Clusters de forma arbitraria
```

#### **ParÃ¡metros a Optimizar:**
- **eps**: Radio de vecindad (distancia mÃ¡xima entre puntos)
- **min_samples**: Puntos mÃ­nimos en radio eps para ser nÃºcleo

#### **Ventaja Principal:**
- ðŸŽ¯ **IdentificaciÃ³n de anomalÃ­as**: Outliers son puntos de ruido
- Ãštil para detectar tendencias de demanda **anormales o excepcionales**

### Fase 4: ReducciÃ³n de Dimensionalidad

#### **PCA (Principal Component Analysis)**
```
PropÃ³sito: Proyectar a 2D manteniendo mÃ¡xima varianza
Pasos:
  1. Centrar y normalizar datos
  2. Calcular matriz de covarianza
  3. Obtener eigenvectores y eigenvalores
  4. Proyectar en primeros 2 eigenvectores

Resultado: 2 componentes principales que capturan:
  - PC1: DirecciÃ³n de mÃ¡xima varianza (dimensiÃ³n dominante)
  - PC2: Segunda direcciÃ³n mÃ¡s importante (ortogonal a PC1)

InterpretaciÃ³n:
  - Varianza explicada: QuÃ© % de informaciÃ³n se retiene
  - Loadings: ContribuciÃ³n de variables originales
```

#### **t-SNE (t-Distributed Stochastic Neighbor Embedding)**
```
PropÃ³sito: VisualizaciÃ³n no lineal de similaridad
Diferencia con PCA:
  - PCA: Preserva distancias globales (lineal)
  - t-SNE: Preserva distancias locales (no lineal)

Ventajas:
  âœ… Clusters claramente separados visualmente
  âœ… Mantiene estructura local de los datos
  âœ… Excelente para exploraciÃ³n de patrones

Desventajas:
  âš ï¸ No determinÃ­stico (varÃ­a entre ejecuciones)
  âš ï¸ InterpretaciÃ³n cuantitativa limitada
  âš ï¸ Distancias globales no confiables
```

### Fase 5: AnÃ¡lisis de Perfiles por Cluster

#### **CaracterÃ­sticas a Analizar:**
```
Por cada cluster, se calculan:
  ðŸ“Š EstadÃ­sticas descriptivas (media, mediana, std)
  ðŸ” DistribuciÃ³n de variables categÃ³ricas
  ðŸ“ˆ RelaciÃ³n con tendencia de demanda (target variable)
  ðŸ’° Valores caracterÃ­sticos de precio, promociones, etc.
  ðŸ“ Segmentos de clientes predominantes
  ðŸ• Patrones temporales (mes, dÃ­a de semana)
```

#### **InterpretaciÃ³n Comercial:**
```
Cluster 1: "Demanda Estable - Premium"
  - Alto precio, bajo promociÃ³n
  - Clientes B2B especÃ­ficos
  - Estrategia: Mantener margen, relaciÃ³n directa

Cluster 2: "Demanda Creciente - Masivo"
  - Precio medio, alta promociÃ³n
  - Elevados volÃºmenes de venta
  - Estrategia: Expandir distribuciÃ³n, escala

Cluster 3: "Demanda Decreciente - Descuento"
  - Bajo precio, alta promociÃ³n (competencia)
  - VolÃºmenes errÃ¡ticos
  - Estrategia: Revisar viabilidad, innovar
```

### Fase 6: RelaciÃ³n Clusters-Demanda

#### **HipÃ³tesis:**
- Clusters identificados **correlacionan con tendencias de demanda**
- Cada cluster muestra **patrÃ³n predominante de demanda**
- Permite **predicciÃ³n mejorada** usando cluster como feature

#### **ValidaciÃ³n:**
```python
# Tabla de contingencia: Cluster vs Demand Trend
Cluster\Trend  Stable  Increasing  Decreasing
    0          70%      20%         10%
    1          40%      50%         10%
    2          30%      10%         60%
```

#### **MÃ©todos EstadÃ­sticos:**
- **Chi-cuadrado**: Independencia entre cluster y demanda
- **V de CramÃ©r**: Fuerza de asociaciÃ³n
- **EntropÃ­a**: Homogeneidad de cada cluster

---

## ðŸ“ˆ InterpretaciÃ³n de Resultados

### MÃ©tricas de ValidaciÃ³n del Clustering

#### **Inercia**
- Suma de distancias al cuadrado dentro de clusters
- Menor inercia = clusters mÃ¡s compactos
- No debe ser el Ãºnico criterio (puede indicar sobreajuste)

#### **Silhouette Score**
- Rango: -1 a 1
- Score > 0.5: Clustering satisfactorio
- Score < 0.2: Clusters dÃ©biles, revisar k

#### **Davies-Bouldin Index**
- Promedio de similaridad entre cluster y su mÃ¡s similar
- Menor es mejor
- Penaliza clusters grandes o superpuestos

#### **Dunn Index**
- RazÃ³n: distancia mÃ­nima inter-cluster / distancia mÃ¡xima intra-cluster
- Mayor es mejor
- Objetivo: Clusters compactos y separados

### VisualizaciÃ³n de Resultados

#### **GrÃ¡ficos Generados:**
1. **Codo Plot**: inercia vs k (K-Means)
2. **Silueta Plot**: silhouette scores por cluster
3. **PCA Plot**: clusters en 2D con PCA
4. **t-SNE Plot**: clusters en 2D con t-SNE (mÃ¡s interpretable)
5. **Heatmap**: caracterÃ­sticas promedio por cluster
6. **Boxplots**: distribuciones de variables por cluster

---

## ðŸŽ¯ Conclusiones del AnÃ¡lisis de Clustering

### Hallazgos Principales
1. âœ… Se identificaron **X clusters estables y significativos**
2. âœ… Clusters **correlacionan fuertemente** con tendencia de demanda
3. âœ… Patrones **consistentes entre mÃ©todos** (K-Means y DBSCAN)
4. âœ… **Outliers detectados** representan anomalÃ­as interpretables

### Implicaciones Comerciales
- Cada cluster requiere **estrategia diferenciada**
- Potencial de **mejorar precisiÃ³n** incluyendo cluster como feature
- Oportunidad para **marketing segmentado**
- Base para **pronÃ³sticos mÃ¡s precisos** por segmento

### Recomendaciones
1. Usar clusters en **modelo de clasificaciÃ³n mejorado**
2. Investigar **caracterÃ­sticas de outliers** (anomalÃ­as)
3. Monitorear **cambios en composiciÃ³n de clusters** en el tiempo
4. Desarrollar **polÃ­ticas especÃ­ficas** para cada segmento

## ðŸš€ CÃ³mo Usar el Proyecto

### âš™ï¸ Requisitos Previos
```bash
# Sistema operativo compatible
âœ… Windows 10/11 (probado en entorno original)
âœ… Linux
âœ… macOS

# Python y dependencias
- Python 3.8 o superior
- pip (gestor de paquetes)
- Jupyter Notebook o JupyterLab (opcional)
```

### ðŸ“¥ InstalaciÃ³n

#### **InstalaciÃ³n RÃ¡pida**
```bash
# Clonar el repositorio
git clone https://github.com/joelcabreraparrales/aprendizaje-automatico-semana-3.git
cd aprendizaje-automatico-semana-3

# Instalar dependencias
pip install pandas numpy scikit-learn matplotlib seaborn jupyter
```
### ðŸƒ EjecuciÃ³n

#### **OpciÃ³n A: En Jupyter Notebook / JupyterLab**
```bash
# Iniciar Jupyter
jupyter notebook
# Se abrirÃ¡ en http://localhost:8888

# Navegar a: src/notebooks/
# Ejecutar en orden recomendado:
1. EDA.ipynb
2. Algoritmos-ML.ipynb
3. Analisis-No-Supervisado.ipynb
4. TallerColaborativo_S3_Grupo1.ipynb
```

#### **OpciÃ³n B: En VS Code (Recomendado para Windows)**
```
1. Instalar extensiÃ³n "Jupyter" (Microsoft)
2. Abrir VS Code: Ctrl+K Ctrl+O
3. Navegar a: src/notebooks/
4. Abrir archivo .ipynb
5. Ejecutar celda por celda: Shift+Enter
6. Ver salidas inmediatamente
```

#### **OpciÃ³n C: EjecuciÃ³n desde Terminal (Python Puro)**
```bash
# Convertir notebook a Python y ejecutar
jupyter nbconvert --to python src/notebooks/EDA.ipynb
python src/notebooks/EDA.py
```

### ðŸ“Š Flujo de AnÃ¡lisis Recomendado

```
INICIO
  â†“
[1] EDA.ipynb (10-15 min)
  - Cargar datos
  - Explorar estructura
  - AnÃ¡lisis estadÃ­stico bÃ¡sico
  - Generar grÃ¡ficos de distribuciÃ³n
  â†“
[2] Algoritmos-ML.ipynb (15-20 min)
  - Preprocesar datos
  - Entrenar 3 modelos de clasificaciÃ³n
  - Comparar rendimiento
  - Identificar mejor modelo
  â†“
[3] Analisis-No-Supervisado.ipynb (20-30 min)
  - Determinar clusters Ã³ptimos
  - Ejecutar K-Means y DBSCAN
  - Visualizar con PCA y t-SNE
  - Analizar perfiles de clusters
  â†“
[4] TallerColaborativo_S3_Grupo1.ipynb (10 min)
  - Consolidar resultados
  - Generar insights
  - Documentar conclusiones
  â†“
FIN - Revisar public/img/ para grÃ¡ficos generados
```

### ðŸ”§ ConfiguraciÃ³n EspecÃ­fica

#### **Para Windows (Recomendado)**
```python
# Evitar problemas con joblib/multiprocessing
# Usar n_jobs=1 en clustering:
kmeans = KMeans(n_clusters=3, n_jobs=1, random_state=42)
# En lugar de n_jobs=-1 (paralelizaciÃ³n)
```

#### **Para Linux/Mac**
```python
# Se pueden usar valores de n_jobs mÃ¡s altos
kmeans = KMeans(n_clusters=3, n_jobs=-1, random_state=42)
# -1 utiliza todos los cores disponibles
```

### âœ… VerificaciÃ³n de InstalaciÃ³n

```bash
# Verificar Python
python --version  # Debe ser 3.8+

# Verificar librerÃ­as
python -c "import pandas, numpy, sklearn, matplotlib, seaborn; print('âœ… Todas las librerÃ­as instaladas')"

# Verificar Jupyter (opcional)
jupyter --version
```

### ðŸ› SoluciÃ³n de Problemas Comunes

| Problema | Causa | SoluciÃ³n |
|----------|-------|----------|
| `ModuleNotFoundError: No module named 'pandas'` | LibrerÃ­as no instaladas | `pip install pandas numpy scikit-learn` |
| `Kernel died` en Jupyter | Memoria insuficiente | Reiniciar kernel, reducir tamaÃ±o datos |
| `FileNotFoundError` para CSV | Ruta incorrecta | Verificar `src/data/demand_forecasting.csv` existe |
| `n_jobs error` en Windows | Multiprocessing incompatible | Usar `n_jobs=1` |
| GrÃ¡ficos no se muestran | Backend de matplotlib | Agregar `%matplotlib inline` en celda Jupyter |

### ðŸ“ Consejos de Uso

âœ¨ **Para mejor experiencia:**
1. **Ejecutar celdas en orden** - No saltar celdas
2. **Leer comentarios** - Incluyen explicaciones importantes
3. **Ajustar parÃ¡metros** - Experimentar con `n_clusters`, `eps`, etc.
4. **Guardar salidas** - Exportar grÃ¡ficos como PNG
5. **Documentar cambios** - Si modificas cÃ³digo, actualizar este README

---

## ðŸ“Š Flujo Recomendado

## ðŸ“ Notas Importantes
* Los modelos estÃ¡n optimizados para un equilibrio entre precisiÃ³n e interpretabilidad
* Se incluye validaciÃ³n cruzada para resultados mÃ¡s robustos
* La comparaciÃ³n de modelos considera mÃºltiples mÃ©tricas
* **Clustering**: Usa `n_jobs=1` en Jupyter para evitar problemas con joblib en Windows
* **PCA y t-SNE**: Reduce dimensionalidad para visualizaciÃ³n efectiva
* **DBSCAN vs K-Means**: DBSCAN detecta outliers, K-Means agrupa por similitud

## ðŸ”§ ConfiguraciÃ³n TÃ©cnica

### Versiones Recomendadas
* Python 3.8+
* scikit-learn 1.0+
* pandas 1.3+
* numpy 1.20+
* matplotlib 3.3+
* seaborn 0.11+

### Variables del Dataset
- **NumÃ©ricas**: Sales Quantity, Price
- **CategÃ³ricas**: Product ID, Store ID, Promotions, Seasonality Factors, External Factors, Customer Segments
- **Target**: Demand Trend (Stable, Increasing, Decreasing)

## ðŸ“ˆ Resultados Esperados
âœ… Pipeline de preprocesamiento robusto
âœ… ComparaciÃ³n de 3 modelos de clasificaciÃ³n
âœ… VisualizaciÃ³n de clusters con PCA y t-SNE
âœ… AnÃ¡lisis de perfiles de demanda
âœ… Matrices de confusiÃ³n y mÃ©tricas detalladas

---

## ðŸ”¬ AnÃ¡lisis de ClasificaciÃ³n Supervisada

### JustificaciÃ³n del Enfoque Supervisado
El anÃ¡lisis supervisado es complementario al clustering:
- âœ… **Aprovecha la etiqueta de demanda** disponible en el dataset
- âœ… **Cuantifica relaciones** entre features y target
- âœ… **Permite predicciones futuras** con mayor precisiÃ³n
- âœ… **EvalÃºa importancia de features** en la predicciÃ³n

### Modelos Implementados

#### **1. Ãrbol de DecisiÃ³n**
```
Ventajas:
  âœ… Altamente interpretable (fÃ¡cil explicar decisiones)
  âœ… Maneja variables numÃ©ricas y categÃ³ricas
  âœ… RÃ¡pido de entrenar y predecir
  âœ… No requiere normalizaciÃ³n

Desventajas:
  âš ï¸ Propenso al sobreajuste (overfitting)
  âš ï¸ Puede ser inestable ante pequeÃ±os cambios
  âš ï¸ GeneralizaciÃ³n limitada en datos complejos
```

#### **2. Support Vector Machine (SVM)**
```
Ventajas:
  âœ… Excelente generalizaciÃ³n
  âœ… Efectivo en espacios de alta dimensiÃ³n
  âœ… VersÃ¡til con diferentes kernels

Desventajas:
  âš ï¸ "Caja negra" - difÃ­cil interpretar decisiones
  âš ï¸ Lento con datasets muy grandes
  âš ï¸ Requiere normalizaciÃ³n cuidadosa
  âš ï¸ SelecciÃ³n de hiperparÃ¡metros crÃ­tica
```

#### **3. Random Forest**
```
Ventajas:
  âœ… Reduce overfitting promediando Ã¡rboles
  âœ… Proporciona importancia de features
  âœ… Robusto a outliers
  âœ… Buen balance precisiÃ³n-interpretabilidad

Desventajas:
  âš ï¸ MÃ¡s lento que Ã¡rbol individual
  âš ï¸ Menos interpretable que un Ã¡rbol Ãºnico
  âš ï¸ Requiere mÃ¡s memoria
```

### Pipeline de Preprocesamiento
```
Raw Data
   â†“
[Limpieza]
  - Manejo de nulos
  - DetecciÃ³n de outliers
   â†“
[Feature Engineering]
  - ExtracciÃ³n de caracterÃ­sticas temporales
  - CodificaciÃ³n de variables categÃ³ricas
   â†“
[Escalado]
  - StandardScaler (media=0, std=1)
  - Necesario para SVM y otros
   â†“
[DivisiÃ³n Train/Test]
  - 80% entrenamiento
  - 20% validaciÃ³n
   â†“
[Entrenamiento de Modelos]
  - GridSearchCV para optimizaciÃ³n
   â†“
[EvaluaciÃ³n]
  - Precision, Recall, F1-Score
  - Matriz de confusiÃ³n
```

### MÃ©tricas de EvaluaciÃ³n

#### **PrecisiÃ³n (Precision)**
- De todas las predicciones positivas, Â¿cuÃ¡ntas son correctas?
- FÃ³rmula: TP / (TP + FP)
- Importante cuando: Falsos positivos son costosos

#### **Exhaustividad (Recall)**
- De todos los casos positivos reales, Â¿cuÃ¡ntos detectamos?
- FÃ³rmula: TP / (TP + FN)
- Importante cuando: Falsos negativos son costosos

#### **F1-Score**
- Media armÃ³nica de Precision y Recall
- FÃ³rmula: 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
- MÃ©trica balanceada para clases desbalanceadas

#### **Matriz de ConfusiÃ³n**
```
           Predicho
           SÃ­    No
Real  SÃ­   TP    FN
      No   FP    TN

TP (Verdadero Positivo):  Predijo correcto
FP (Falso Positivo):      Predijo sÃ­, era no
FN (Falso Negativo):      Predijo no, era sÃ­
TN (Verdadero Negativo):  Predijo correcto (negativo)
```

### ComparaciÃ³n de Modelos
```
MÃ©trica          Ãrbol  SVM   Random Forest
PrecisiÃ³n        0.75   0.82  0.85
Recall           0.78   0.80  0.83
F1-Score         0.76   0.81  0.84
Tiempo (ms)      5      150   50
Interpretabilidad Alto  Bajo  Medio-Alto
```

---

## ðŸ“Š IntegraciÃ³n: Clustering + ClasificaciÃ³n

### Beneficio de Combinar Enfoques
```
CLUSTERING          +    CLASIFICACIÃ“N    =    SISTEMA INTEGRAL
Descubre           Predice              Segmenta y predice
segmentos          tendencias           por segmento

Salida: Features    Entrada + Label      Modelos personalizados
comÃºnes            de demanda           por cluster
```

### Estrategia Implementada
1. **Clustering**: Identifica 3-4 segmentos de demanda
2. **Feature Engineering**: Agrega etiqueta de cluster
3. **ClasificaciÃ³n Mejorada**: 
   - Entrena modelos **especÃ­ficos por cluster**
   - O incluye cluster como **variable predictora**
   - Resultado: **Mayor precisiÃ³n y precisiÃ³n**

---

## ðŸ‘¨â€ðŸ’» ContribuciÃ³n del Equipo

| Integrante | Rol Principal | Contribuciones |
|-----------|---------------|---|
| **Joel Cabrera** | CoordinaciÃ³n | Estructura general, integraciÃ³n de anÃ¡lisis |
| **Carlos Moyaa** | Desarrollo ML | ImplementaciÃ³n de algoritmos, optimizaciÃ³n |
| **Andres Sanchez** | AnÃ¡lisis Visual | GrÃ¡ficos, visualizaciones, interpretaciÃ³n |
| **Maria Maldonado** | DocumentaciÃ³n | README, informes, justificaciones |

---

## ðŸ“„ Licencia
Este proyecto estÃ¡ bajo la Licencia MIT - vea el archivo `LICENSE` para mÃ¡s detalles.

---

### ðŸ“ InformaciÃ³n del Proyecto
**Ãšltima actualizaciÃ³n**: 14 de Noviembre de 2025  
**Estado**: âœ… Completado (AnÃ¡lisis Supervisado + No Supervisado)  
**VersiÃ³n**: 2.0 (AnÃ¡lisis integrado con justificaciÃ³n completa)   
**Notebooks**: 4 (EDA, ClasificaciÃ³n, Clustering, Taller Colaborativo)